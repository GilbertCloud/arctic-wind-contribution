{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca749452-87e7-4275-b29a-999654360561",
   "metadata": {},
   "source": [
    "# Analysis notebook for PiC_UVnudge runs\n",
    "## Set up\n",
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec0c162-355c-46a6-828a-f941fe06e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats, interpolate\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.mathtext import _mathtext as mathtext\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import gridspec, animation\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from cmcrameri import cm\n",
    "from Processing_functions import FixLongitude, Wilks_pcrit\n",
    "import jinja2\n",
    "import cftime\n",
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19ee4e5-8c0d-45bc-b5bd-1de946672acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot types to make - CHANGE\n",
    "# 0: Weighted spatial mean or Arctic sea ice area\n",
    "# 1: Spatial maximum (AMOC)\n",
    "# 2: Leave alone (doing spatial map or sea ice concentration)\n",
    "# 3: Zonal\n",
    "plots = {\n",
    "    'amoc': [False, 1],\n",
    "    'toa': [False, 0],\n",
    "    'map': [False, 2],\n",
    "    'ts': [False, 0],\n",
    "    'sia': [False, 0],\n",
    "    'mtrd': [False, 0],\n",
    "    'strd': [True, 2],\n",
    "    'zon': [False, 3],\n",
    "    'ztrd': [False, 3]\n",
    "}\n",
    "\n",
    "## Categorical plot type - DO NOT CHANGE\n",
    "plot_types = {\n",
    "    'spatial': [False, []],\n",
    "    'line': [False, []],\n",
    "    'mtrd': [False, []],\n",
    "    'zonal': [False, []]\n",
    "}\n",
    "\n",
    "# Set up plot_types based on plots\n",
    "for pl, att in plots.items():\n",
    "    if att[0]:\n",
    "        if pl == 'mtrd':\n",
    "            plot_types['mtrd'][0] = True\n",
    "            plot_types['mtrd'][1].append(pl)\n",
    "        elif att[1] <= 1:\n",
    "            plot_types['line'][0] = True\n",
    "            plot_types['line'][1].append(pl)\n",
    "        elif att[1] == 2:\n",
    "            plot_types['spatial'][0] = True\n",
    "            plot_types['spatial'][1].append(pl)\n",
    "        elif att[1] == 3:\n",
    "            plot_types['zonal'][0] = True\n",
    "            plot_types['zonal'][1].append(pl)\n",
    "\n",
    "# Spatial domain - CHANGE s_domain & t_domain only\n",
    "s_domain = False # True: Global, False: Arctic\n",
    "s_domain = True if (plots['toa'][0] or plots['amoc'][0]) else s_domain # Make sure TOA is global domain\n",
    "a_domain = plot_types['spatial'][0] or plot_types['zonal'][0] # True: 50-90, False: 70-90\n",
    "t_domain = 1980 # start year\n",
    "\n",
    "## Time averaging type - CHANGE\n",
    "time_avg = 2   # 0: Monthly, 1: Yearly, 2: Seasonal, 3: All data, 4: Timeseries\n",
    "\n",
    "## Ensemble mean or All members - CHANGE\n",
    "ens_type = 1   # 0: All_members, 1: Mean\n",
    "\n",
    "# Variables - CHANGE\n",
    "comp = 'ice' # compset\n",
    "freq = 0 # 0: monthly, 1: daily\n",
    "var_ind = 0\n",
    "\n",
    "# DO NOT CHANGE\n",
    "var_list = {'atm': ['TREFHT','PSL','RESTOM','Z3'],\n",
    "            'ice': ['aice'],\n",
    "            'ocn': ['MOC']}\n",
    "var_ext = {0: '', 1: '_d'}\n",
    "var = var_list[comp][var_ind]+var_ext[freq]\n",
    "\n",
    "# Plot levels for spatial trends - CHANGE\n",
    "plot_levels = [300, 500, 850, 925]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d169a33-b96d-4a37-a482-5ed2e4babe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test numbers - DO NOT CHANGE\n",
    "tst_nums = np.arange(1,4)\n",
    "\n",
    "## Test names\n",
    "# O: LENS ensemble\n",
    "# 1: PiC_UVnudge ensemble\n",
    "# 2: PiC_UVnudge single run\n",
    "# 3: observations\n",
    "# attribute structure: [use dataset, dataset type, line color, line style, zorder]\n",
    "# CHANGE ONLY - use dataset\n",
    "ds_names = {\n",
    "    'LENS2 piControl': [True, 0],\n",
    "    'PiC_UVnudge': [False, 1],\n",
    "    'PiC_UVnudge_LM': [False, 1],\n",
    "    'PiC_UVnudge_MM': [False, 1],\n",
    "    'PiC_UVnudgenew': [False, 2],\n",
    "    'PiC_UVnudge_2006': [True, 1],\n",
    "    'PiC_UVnudge_LM2006': [True, 1],\n",
    "    'PiC_UVnudge_MM2006': [True, 1],\n",
    "    'PiC_UVnudge_1988': [False, 2],\n",
    "    'PiC_UVnudge_2006_2000': [False, 2],\n",
    "    'ERA5': [not (plots['amoc'][0] or plots['toa'][0]), 3],\n",
    "    'GISTEMP': [plots['ts'][0] or (plots['mtrd'][0] and var == 'TREFHT'), 3]\n",
    "}\n",
    "vercompres = 'b.e21.B1850cmip6.f09_g17.'\n",
    "cesm2piC = 'b.e21.B1850.f09_g17.'\n",
    "\n",
    "## Filepaths - DO NOT CHANGE\n",
    "path_to_work = '/glade/work/glydia/'\n",
    "path_to_lensdata = path_to_work+'processed_CESM2_LENS_data/'\n",
    "path_to_expdata = path_to_work+'Arctic_controls_processed_data/'\n",
    "path_to_plotdata  = path_to_expdata+'plotting_data/'\n",
    "\n",
    "# Extensions - DO NOT CHANGE\n",
    "h_ext = {'atm': ['.h0.'],\n",
    "       'ice': ['.h.','.h1.'],\n",
    "       'ocn': ['.h.']}\n",
    "yr_extn = {False: [\".195001-202312.\",\".19500101-20231231.\"],\n",
    "           True: [\".*.\",\".05010101-05741231.\"]}\n",
    "vert_lev = {'atm': [False,False,False,True],\n",
    "            'ice': [False],\n",
    "            'ocn': [False]}\n",
    "file_bool = not vert_lev[comp][var_ind] and freq == 0\n",
    "file_ext = {True: 'nc', False: 'zarr'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4292bdd7-cfa3-4313-b4cc-2f41e01099d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## DO NOT CHANGE ANYTHING BELOW THIS LINE #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62938a12-2e19-4666-aa09-c5fa7d6a5ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "## Select plot type\n",
    "time_str_list = {0: 'month', 1: 'year', 2: 'season', 3: 'all', 4: 'timeseries'}\n",
    "time_outstr = time_str_list[time_avg]\n",
    "\n",
    "## Select ensemble type\n",
    "ens_str_list = {0: 'All_members', 1: 'Mean'}\n",
    "ens_str = ens_str_list[ens_type]\n",
    "\n",
    "## Select time and spatial domain strings\n",
    "sd_str_list = {True: 'Global', False: 'Arctic'}\n",
    "sd_str = sd_str_list[s_domain]\n",
    "td_str = str(t_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c037b32-b950-49fe-8053-a7b0f0890301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot types: \n",
      "   spatial\n",
      "      strd\n",
      "Variable(s): aice\n",
      "Datasets:\n",
      "   LENS2 piControl\n",
      "   PiC_UVnudge_2006\n",
      "   PiC_UVnudge_LM2006\n",
      "   PiC_UVnudge_MM2006\n",
      "   ERA5\n",
      "Spatial domain: Arctic\n",
      "Time domain: 1980-2023\n",
      "Time averaging: season\n",
      "Ensemble averaging: Mean\n"
     ]
    }
   ],
   "source": [
    "## Print Script Configurations\n",
    "# Plot types\n",
    "print('Plot types: ')\n",
    "catcount = 0\n",
    "\n",
    "for cat, att in plot_types.items():\n",
    "    if att[0]:\n",
    "        print('   '+cat)\n",
    "        catcount += 1\n",
    "        for pl in att[1]:\n",
    "            print('      '+pl)\n",
    "\n",
    "# Variable\n",
    "if var == 'Z3':\n",
    "    print('Variable: Z3, U, V')\n",
    "else:\n",
    "    print('Variable(s): '+var)\n",
    "\n",
    "# Datasets\n",
    "print('Datasets:')\n",
    "for dsname, attr in ds_names.items():\n",
    "    if attr[0]:\n",
    "        print('   '+dsname)\n",
    "\n",
    "# Spatial domain\n",
    "print('Spatial domain: '+sd_str)\n",
    "\n",
    "# Time domain\n",
    "print('Time domain: '+td_str+'-2023')\n",
    "\n",
    "# Time averaging\n",
    "print('Time averaging: '+time_outstr)\n",
    "\n",
    "# Ensemble\n",
    "print('Ensemble averaging: '+ens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480b0069-cf20-4d2e-bcbe-048b55a0fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check number of different averaging types\n",
    "if catcount > 1:\n",
    "    raise SystemExit(\"ERROR: More than one averaging type fed into CreateMasterDS()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2f36b-80ac-4dc4-bb06-4ccbf2ea8b8e",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2b079e-c454-47b5-9f39-da2604737b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = PBSCluster(cores    = 1,\n",
    "                     memory   = '50GiB',\n",
    "                     queue    = 'casper',\n",
    "                     walltime = '12:00:00',\n",
    "                     account  = 'UCUB0155',\n",
    "                     name='PiC_UVnudge_process_'+var)\n",
    "cluster.scale(4*9)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efcbec8d-3070-48ea-b283-4604a6ef7957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-a5f24e69-1a35-11f0-b506-3cecef1b12de</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/glydia/Arctic_breakdown/proxy/36821/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/glydia/Arctic_breakdown/proxy/36821/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"https://jupyterhub.hpc.ucar.edu/stable/user/glydia/Arctic_breakdown/proxy/36821/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">PiC_UVnudge_process_aice</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/glydia/Arctic_breakdown/proxy/36821/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/glydia/Arctic_breakdown/proxy/36821/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-bc7297e2-55c2-40d1-b699-43163a0149a8</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.208.97:43471\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/glydia/Arctic_breakdown/proxy/36821/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/glydia/Arctic_breakdown/proxy/36821/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.208.97:43471' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e55d42-5af5-48f9-b26a-b9a0f93a4770",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c61464-ad0a-428f-82e5-0b561d3c9434",
   "metadata": {},
   "source": [
    "#### pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513f1625-7b24-4ff2-b668-ce9b5beec04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(da):\n",
    "    da = da.assign_coords(time=pd.date_range('1950-01-01','2023-12-01',freq='MS'))\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b41d4-d8d5-4c29-993a-3c2978eb0a2f",
   "metadata": {},
   "source": [
    "#### LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bf5069-532f-4252-9bbc-24350643b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(casename, set_type, varname):\n",
    "    # Condition for only using 501-574 from LENS2 piControl, i.e. when plotting Z3\n",
    "    cond_lens = var != 'Z3' # True means use 50 slices, False means use 501-574 only\n",
    "    \n",
    "    # Create file name\n",
    "    if set_type == 0:\n",
    "        # If cond_lens true, select all netcdf files with *\n",
    "        # Else, select zarr file with 1950-2023 time string\n",
    "        filename = cesm2piC+'CMIP6-piControl.001'+h_ext[comp][freq]+varname+yr_extn[cond_lens][freq]+file_ext[file_bool]\n",
    "        totalpath = path_to_lensdata+filename\n",
    "    elif set_type == 3:\n",
    "        filename = casename+h_ext[comp][freq]+varname+yr_extn[False][freq]+file_ext[file_bool]\n",
    "        totalpath = path_to_work+'processed_'+casename+'_data/'+filename\n",
    "    else:\n",
    "        filename = vercompres+casename+h_ext[comp][freq]+varname+yr_extn[False][freq]+file_ext[file_bool]\n",
    "        totalpath = path_to_expdata+'processed_'+casename+'_data/'+filename\n",
    "\n",
    "    # Load if piControl and using slices\n",
    "    if set_type == 0 and cond_lens: \n",
    "        data = xr.open_mfdataset(totalpath, combine='nested',concat_dim='slice',preprocess=pre_process)  \n",
    "    \n",
    "    # Load if netCDF and not piControl\n",
    "    elif file_bool:\n",
    "        data = xr.open_dataset(totalpath, chunks={'time':12})  \n",
    "\n",
    "    # Load if Zarr\n",
    "    else:\n",
    "        data = xr.open_zarr(totalpath, group=varname,  chunks={'time':12})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e72694-3aa4-4b42-b3ea-20ee4de4db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set spatial domain slicing\n",
    "slice_ocn = dict(lat_aux_grid=slice(33,55),moc_z=slice(800*100,2200*100), moc_comp=0,transport_reg=1)\n",
    "slice_icemod = dict(nj=slice(250,385))\n",
    "slice_iceobs = dict(lat=slice(50,90))\n",
    "slice_atmwei = dict(lat=slice(70,90))\n",
    "slice_atmspt = dict(lat=slice(50,90))\n",
    "slice_time = dict(time=slice(td_str+'-01-01','2023-12-31'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8d6a3-0ddd-428c-8840-fb41ad700abe",
   "metadata": {},
   "source": [
    "#### CreateMasterDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6485574e-4a76-4506-b2ba-e3e00cc8b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMasterDS(varname):\n",
    "    ds_load_list = []\n",
    "    pd_time = pd.date_range('1950-01-01','2023-12-01',freq='MS')\n",
    "\n",
    "    # Find which plots will be made\n",
    "    funcstr = None\n",
    "    for plotname, attrs in plots.items():\n",
    "        # If plot will be made\n",
    "        if attrs[0]:\n",
    "            # If weighted spatial average or Arctic sea ice area\n",
    "            if attrs[1] == 0:\n",
    "                funcstr = 'S' if varname == 'aice' else 'W'\n",
    "                break\n",
    "            # Elif spatial maximum\n",
    "            elif attrs[1] == 1:\n",
    "                funcstr = 'M'\n",
    "                break\n",
    "\n",
    "            # Elif zonal average\n",
    "            elif attrs[1] == 3:\n",
    "                funcstr = 'Z'\n",
    "                break\n",
    "\n",
    "    print('Figure out processing calculations to do')\n",
    "\n",
    "    ## Load all datasets\n",
    "    # Load each dataset\n",
    "    for dsname, attrs in ds_names.items():\n",
    "        # If dataset is going to be plotted\n",
    "        if attrs[0]:\n",
    "            print(dsname)\n",
    "            # If dataset is ERA5\n",
    "            if dsname == 'ERA5':\n",
    "                # If using U or V variable, use Target_U & Target_V from PiC_UVnudge\n",
    "                if varname == 'U' or varname == 'V':\n",
    "                    varname = 'Target_'+varname\n",
    "                    ds_load = LoadData('PiC_UVnudge', 1, varname)\n",
    "                    ds_load = ds_load.mean('ensemble_member')\n",
    "                else:\n",
    "                    ds_load = LoadData(dsname, attrs[1], varname)\n",
    "                    ds_load = CalcGridArea(ds_load)\n",
    "            else:\n",
    "                ds_load = LoadData(dsname, attrs[1], varname)\n",
    "    \n",
    "            # Reduce to dataarray\n",
    "            ds_load = ds_load[varname]\n",
    "        \n",
    "            # Rename dataarray name to be casename\n",
    "            ds_load = ds_load.rename(dsname)\n",
    "            \n",
    "            print('  Initial data loading complete')\n",
    "\n",
    "            # Make all datasets have the same time axis\n",
    "            ds_load = ds_load.assign_coords(time=pd_time)\n",
    "\n",
    "            print('  Fixed time dimension')\n",
    "                \n",
    "\n",
    "            ## Do weighted averages, ensemble means, sea ice area calculations, and area maximums\n",
    "            # Do spatial domain slicing\n",
    "            # ICE\n",
    "            if comp == 'ice':\n",
    "                ds_load = ds_load.loc[slice_iceobs] if attrs[1] == 3 else ds_load.loc[slice_icemod]\n",
    "            # OCEAN\n",
    "            elif comp == 'ocn':\n",
    "                ds_load = ds_load.loc[slice_ocn].drop('moc_components')\n",
    "            # ATMOSPHERE\n",
    "            else: \n",
    "                if not s_domain and a_domain:\n",
    "                    ds_load = ds_load.loc[slice_atmspt]\n",
    "                elif not s_domain and not a_domain:\n",
    "                    ds_load = ds_load.loc[slice_atmwei]\n",
    "\n",
    "                # Remove cyclic point if not ERA5\n",
    "                if dsname != 'ERA5':\n",
    "                    ds_load = ds_load.where(ds_load.lon != 180., drop=True)\n",
    "\n",
    "            print('  Sliced data')\n",
    "                    \n",
    "            # Sort processing by variable and graph type\n",
    "            # Doing spatial plot or SIC\n",
    "            if funcstr == None or funcstr == 'Z':\n",
    "                print('  No proceesing, spatial')\n",
    "                # If ERA5, rename lat/lon\n",
    "                if dsname == 'ERA5':\n",
    "                    ds_load = ds_load.rename({'lat':'latE', 'lon':'lonE'})\n",
    "                    if varname == 'Z3':\n",
    "                        ds_load = ds_load.rename({'lev':'plev'})\n",
    "\n",
    "                if funcstr == 'Z':\n",
    "                    print('  Calculating zonal average')\n",
    "                    if dsname == 'ERA5':\n",
    "                        ds_load = ds_load.mean('lonE', skipna=True)\n",
    "                    else:\n",
    "                        ds_load = ds_load.mean('lon', skipna=True)\n",
    "                else:\n",
    "                    print('  No proceesing, spatial')\n",
    "                \n",
    "                    \n",
    "            # Doing SIA plot(s)\n",
    "            elif funcstr == 'S':\n",
    "                print('  Calculating sea ice area')\n",
    "                ds_load = CalcSIA(ds_load, attrs[1])\n",
    "            # Doing ts or TOA plot(s)\n",
    "            elif funcstr == 'W':\n",
    "                print('  Calculating weighted average')\n",
    "                ds_load = CalcWeightedMean(ds_load)\n",
    "            # Doing AMOC plots:\n",
    "            elif funcstr == 'M':\n",
    "                print('  Calculating maximum')\n",
    "                ds_load = ds_load.max(('moc_z','lat_aux_grid'))\n",
    "                \n",
    "            # If Ensemble mean\n",
    "            if ens_type and attrs[1] == 1:\n",
    "                print('  Calculating ensemble mean')\n",
    "                ds_load = ds_load.mean('ensemble_member')\n",
    "\n",
    "            ds_load = ds_load.rename(dsname)\n",
    "\n",
    "            print('  Processing on dataset complete')\n",
    "            ds_load_list.append(ds_load)\n",
    "    \n",
    "    # Merge dataarrays\n",
    "    ds_proc = xr.merge(ds_load_list)\n",
    "\n",
    "    print('All datasets merged')\n",
    "\n",
    "    # Add LENS2 ensemble mean if not 3D variable\n",
    "    ds_proc['LENS2 piControl mean'] = ds_proc['LENS2 piControl'].mean('slice')\n",
    "\n",
    "    # Slice time\n",
    "    ds_proc = ds_proc.loc[slice_time]\n",
    "    \n",
    "    return ds_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350b619-92cf-4723-993d-9c2681bd6e21",
   "metadata": {},
   "source": [
    "#### CalcWeightedMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4806ab-c49b-42fc-bae6-06671098a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcWeightedMean(ds):\n",
    "    # Set up\n",
    "    avg_dim = ('lon','lat')\n",
    "\n",
    "    # Create weights\n",
    "    weights = np.cos(np.deg2rad(ds.lat))\n",
    "    weights.compute()\n",
    "\n",
    "    # Weight data\n",
    "    ds_w = ds.weighted(weights)\n",
    "\n",
    "    # Calculate weighted mean\n",
    "    ds_mean_w = ds_w.mean(avg_dim, skipna=True)\n",
    "\n",
    "    ds_mean_w.compute()\n",
    "    \n",
    "    return ds_mean_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67695ba-005a-4b5d-8a57-d8a6d90d0752",
   "metadata": {},
   "source": [
    "#### CalcAnom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65cd1102-60a8-49b8-9a79-7ac77a95b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcAnom(ds_mean_w, period):\n",
    "    # Calculate period mean\n",
    "    ds_p_mean_w = ds_mean_w.mean(period,skipna=True)\n",
    "\n",
    "    # Calculate anomalies\n",
    "    ds_anom_w = ds_mean_w-ds_p_mean_w\n",
    "\n",
    "    return ds_anom_w       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3d6d8-8ebc-4eed-9f5b-49e6a03f6ccd",
   "metadata": {},
   "source": [
    "#### CalcDetAnom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80803f62-1bec-4d6c-8679-970aeaa6303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcDetAnom(ds_anom_w, period):\n",
    "    if period == 'time':\n",
    "        raw_time = ds_anom_w.time  \n",
    "        ds_anom_w = AddCoordTrend(ds_anom_w, period)\n",
    "        \n",
    "    # Calculate linear regression coefficients\n",
    "    ds_reg_coef = ds_anom_w.polyfit(dim=period,deg=1)\n",
    "\n",
    "    # Calculate y-values of linear regression\n",
    "    ds_yreg = (ds_reg_coef.loc[dict(degree=1)]*ds_anom_w[period])+ds_reg_coef.loc[dict(degree=0)]\n",
    "\n",
    "    # Calculate detrended anomalies\n",
    "    ds_dtrd_anom = ds_anom_w-ds_yreg['polyfit_coefficients']\n",
    "\n",
    "    if period == 'time':\n",
    "        ds_dtrd_anom = ds_dtrd_anom.assign_coords(time=raw_time)\n",
    "    \n",
    "    return ds_dtrd_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe8ecf-4363-4d49-80d9-50d920b87501",
   "metadata": {},
   "source": [
    "#### CalcR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1289081f-a542-4d89-8ca1-eb1baeed9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcR(ds_obs, ds_mod, period, ens):\n",
    "    if ens == 1:\n",
    "        # Calculate model ensemble mean\n",
    "        ds_mod_mean = ds_mod.mean('ensemble_member')\n",
    "    elif ens == 2:\n",
    "        ds_mod_mean = ds_mod\n",
    "    \n",
    "    # Calculate correlation coefficient for anomalies (detrended and not)\n",
    "    ds_r = xr.corr(ds_obs, ds_mod_mean, dim=period)\n",
    "    \n",
    "    return ds_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfdf28-5536-436d-93f9-352bddb6f089",
   "metadata": {},
   "source": [
    "#### CalcEnsSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc79aed-2b54-4198-9421-eb38ec5cf9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcEnsSp(ds_mean_w):\n",
    "    # Calculate ensemble spread\n",
    "    ds_ensp = np.sqrt(ds_mean_w.var('ensemble_member'))\n",
    "\n",
    "    return ds_ensp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eadb602-4df0-4022-8dad-dff010f9d7f2",
   "metadata": {},
   "source": [
    "#### DropNonPiC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d8d4e9-32d2-472b-ad71-03a9dd0569b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropNonPiC(ds):\n",
    "    # Drop all variables and dimensions not used by PiC_UVnudge\n",
    "    var_set = set(ds.keys())\n",
    "\n",
    "    non_pic_vars = []\n",
    "    non_pic_dims = []\n",
    "    for v in var_set:\n",
    "        if 'PiC_UVnudge' not in v:\n",
    "            non_pic_vars.append(v)\n",
    "\n",
    "            if v == 'LENS2 piControl':\n",
    "                non_pic_dims.append('slice')\n",
    "        \n",
    "    ds_d = ds.drop_vars(non_pic_vars)\n",
    "    ds_d = ds_d.drop_dims(non_pic_dims)\n",
    "\n",
    "    return ds_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d1159e-b9ae-417d-a246-b29b1275a5c1",
   "metadata": {},
   "source": [
    "#### NudgeYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cb86b12-ffc0-4872-b72d-01cad313a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NudgeYears(ds_puv, time_type):\n",
    "    numyr_range = np.arange(0.0,74.0)\n",
    "    numoffset = {'PiC_UVnudge': 0.0,\n",
    "                 'PiC_UVnudge_LM': 0.0,\n",
    "                 'PiC_UVnudge_MM': 0.0,\n",
    "                 'PiC_UVnudge_2006': 56.0,\n",
    "                 'PiC_UVnudge_LM2006': 56.0,\n",
    "                 'PiC_UVnudge_MM2006': 56.0,\n",
    "                 'PiC_UVnudgenew': 0.0,\n",
    "                 'PiC_UVnudge_1988': 38.0,\n",
    "                 'PiC_UVnudge_2006_2000': 106.0}\n",
    "    \n",
    "    # Re-assign time coordinate to be number of years nudged\n",
    "    yrsnud_list = []\n",
    "    for dsname, da in ds_puv.items():\n",
    "        if dsname in numoffset.keys():\n",
    "            da_yrsnud = da.assign_coords({time_type: numyr_range+numoffset[dsname]})\n",
    "            yrsnud_list.append(da_yrsnud.rename(dsname))\n",
    "\n",
    "    ds_yrsnud = xr.merge(yrsnud_list)\n",
    "    return ds_yrsnud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371feaf-78bf-448c-adca-c9d6ac40fede",
   "metadata": {},
   "source": [
    "#### CalcMonthTrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc9b37a3-eb45-40a9-a13c-a7e4b3358b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcMonthTrd(weighted_avg):\n",
    "    # Group by month & calculate trends\n",
    "    grouped = weighted_avg.groupby('time.month')\n",
    "\n",
    "    ds_list = []\n",
    "    for mon, dsmon in grouped:\n",
    "        ds_sizes = dsmon.sizes\n",
    "        new_time = np.arange(1,ds_sizes['time']+1)\n",
    "        \n",
    "        dsmon = dsmon.assign_coords(time=new_time)\n",
    "        dsmon = dsmon.polyfit(dim='time',deg=1)\n",
    "        dsmon *= 10\n",
    "        ds_list.append(dsmon)\n",
    "\n",
    "    month_trd = xr.concat(ds_list, pd.Index(np.arange(1,13),name='month'))\n",
    "    \n",
    "    return month_trd['polyfit_coefficients'].loc[dict(degree=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d60a64-dce4-4d93-af6f-bdec71aef528",
   "metadata": {},
   "source": [
    "#### CalcAnnTrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e2d44b8-dbe6-4d9d-a0f9-ecfb620a2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcAnnTrd(weighted_avg):\n",
    "    # Group by year & get annual means\n",
    "    ann_avg = weighted_avg.groupby('time.year').mean('time')\n",
    "\n",
    "    # Calculate annual trend\n",
    "    ann_trd = ann_avg.polyfit(dim='year',deg=1)\n",
    "    ann_trd *= 10\n",
    "    \n",
    "\n",
    "    return ann_trd['polyfit_coefficients'].loc[dict(degree=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e33c71-aef2-4033-870a-865c603f5805",
   "metadata": {},
   "source": [
    "#### Regrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a597fb-2fae-473f-a84c-fa6cac79e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regrid(ds_timeavg, regridder, regrid_type, pvals=False):\n",
    "    # regrid_type: 'sic' or 'era'\n",
    "    sic_cond = (regrid_type == 'sic')\n",
    "    era_cond = (regrid_type == 'era')\n",
    "\n",
    "    if sic_cond:\n",
    "        print('Regridding CICE grid -> ATM grid...')\n",
    "    if era_cond:\n",
    "        print('Regridding ERA5 grid -> ATM grid...')\n",
    "\n",
    "    # Do fillna if regridding sic\n",
    "    if sic_cond:\n",
    "        nval = 0.99 if pvals else 0.000001\n",
    "\n",
    "    # If regridding ERA5 DataArray\n",
    "    if era_cond and type(ds_timeavg) == xr.core.dataarray.DataArray:\n",
    "        da = ds_timeavg.rename({'latE': 'lat', 'lonE': 'lon'})\n",
    "        da_re = regridder(da)\n",
    "        da_re = da_re.rename({'x':'lon', 'y':'lat'})\n",
    "        da_regrid = da_re.rename('ERA5')\n",
    "\n",
    "        return da_regrid\n",
    "        \n",
    "\n",
    "    # Else, regridding dataset\n",
    "    else:\n",
    "        # Regrid data\n",
    "        regrid_list = []\n",
    "        for dsname, da in ds_timeavg.items():\n",
    "            cond = {'sic': dsname == 'ERA5',\n",
    "                    'era': dsname != 'ERA5'}\n",
    "            # If SIC regridding & dataset name is ERA5, don't regrid ERA5\n",
    "            # If ERA regridding & dataset name is not ERA5, don't regrid, just append\n",
    "            if cond[regrid_type]:\n",
    "                # Don't regrid, just add for era regridding\n",
    "                if era_cond:\n",
    "                    da = da.assign_coords({'lon':lons, 'lat': lats})\n",
    "                    regrid_list.append(da.rename(dsname))\n",
    "            # Ignore pcrit variables\n",
    "            elif ' pcrit' in dsname:\n",
    "                regrid_list.append(da)\n",
    "            \n",
    "            else:\n",
    "                print('Regridding '+dsname)\n",
    "    \n",
    "                # Need to rename for era\n",
    "                if era_cond:\n",
    "                    da = da.rename({'latE': 'lat', 'lonE': 'lon'})\n",
    "                da_re = regridder(da)\n",
    "        \n",
    "                # Rename x, y to be lon, lat and reassign lon and lat data (which includes cyclic point)\n",
    "                da_re = da_re.rename({'x':'lon', 'y':'lat'})\n",
    "    \n",
    "                # Only fillna for sic\n",
    "                if sic_cond:\n",
    "                    da_re = da_re.assign_coords({'lon':lons, 'lat': lats})\n",
    "                    da_re = da_re.fillna(nval)\n",
    "                    \n",
    "                regrid_list.append(da_re.rename(dsname))\n",
    "    \n",
    "        # Fill na for ERA5 only if sic & not pvals \n",
    "        if not pvals and sic_cond:\n",
    "            regrid_list.append(ds_timeavg['ERA5'].fillna(nval))\n",
    "        \n",
    "        ds_regrid = xr.merge(regrid_list, join='left')\n",
    "    \n",
    "        return ds_regrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647f9ee-952b-4867-abad-da3e340c864f",
   "metadata": {},
   "source": [
    "#### CalcGridArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab5e1d77-29f0-470a-a67e-6ee7f08f2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcGridArea(ds):\n",
    "    # For ERA5 0.25x0.25 grid\n",
    "    dlat = 0.25\n",
    "    dlon = 0.25\n",
    "    R = 6367.47 # km\n",
    "\n",
    "    lons, lats = np.meshgrid(ds.lon.values, ds.lat.values)\n",
    "\n",
    "    dy = R*np.deg2rad(dlat)\n",
    "    dx = np.deg2rad(dlon)*R*np.cos(np.deg2rad(lats))\n",
    "\n",
    "    area = np.abs(dx*dy)\n",
    "    ds = ds.assign_coords(area=(('lat','lon'), area))\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a8f23-39c3-4c73-b3a5-073a8f2125df",
   "metadata": {},
   "source": [
    "#### CalcSIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfc27f96-e5c7-4984-8c82-0e9e022a66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcSIA(ds, set_type):\n",
    "    # Extracts aice variable and only counts cells with sic > 15%\n",
    "    ds_aice = ds\n",
    "    ds_aice = xr.where(ds_aice > .15,1,0)\n",
    "\n",
    "    # Multiples selected sic cells with tarea, then only selects Arctic sea ice, sums over the entire domain, then converts to km2\n",
    "    if set_type == 3:\n",
    "        dsa = (ds_aice*ds['area']).sum(dim=['lon','lat'])*1e-6\n",
    "    else:\n",
    "        dsa = (ds_aice*ds['tarea']).sum(dim=['ni','nj'])*1e-6*1e-6\n",
    "\n",
    "    # Modifies attributes of DataArray accordingly\n",
    "    dsa.attrs['units'] = 'million km^2'\n",
    "    dsa.attrs['long_name'] = 'Arctic sea ice area'\n",
    "\n",
    "    # Assigns new variable to dataset and returns original dataset\n",
    "    return dsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d4061-559d-4245-8874-9b4cde49bd9b",
   "metadata": {},
   "source": [
    "#### CalcTrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3416dee1-d558-4dc6-80b4-397ee8acfeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcTrend(da, time_type):\n",
    "    slope, intercept, rval, pval, stderr = xr.apply_ufunc(stats.linregress,\n",
    "                                                          da[time_type], da,\n",
    "                                                          input_core_dims=[[time_type], [time_type]],\n",
    "                                                          output_core_dims=[[],[],[],[],[]],\n",
    "                                                          vectorize=True,\n",
    "                                                          dask='parallelized',\n",
    "                                                          dask_gufunc_kwargs=dict(allow_rechunk=True))\n",
    "    slope = slope*10\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f7bc0-56f7-4cd4-805d-4eed6436088e",
   "metadata": {},
   "source": [
    "#### AddCoordTrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d113c47-e6ae-4f26-8342-a910fa84baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddCoordTrend(da, time_type):\n",
    "    da_sizes = da.sizes\n",
    "    new_time = np.arange(1,da_sizes[time_type]+1)\n",
    "    if time_type == 'time':\n",
    "        da = da.assign_coords(time=new_time)\n",
    "    elif time_type == 'year':\n",
    "        da = da.assign_coords(year=new_time)\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7e0de-f588-464c-92aa-ab7c26877816",
   "metadata": {},
   "source": [
    "#### AddCyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "597ab0e8-bc28-4634-9969-6c00225569f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddCyclic(da: xr.DataArray, londim) -> xr.DataArray:\n",
    "    # Add cyclic point\n",
    "    cyclic_data, cyclic_lon = add_cyclic_point(da.data, coord=da[londim])\n",
    "    cyclic_coords = {dim: da.coords[dim] for dim in da.dims}\n",
    "    cyclic_coords[londim] = cyclic_lon\n",
    "\n",
    "    da = xr.DataArray(cyclic_data, dims=da.dims, coords=cyclic_coords, attrs=da.attrs, name=da.name)\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb68ea5-0692-489a-9e02-6eb83d55b66a",
   "metadata": {},
   "source": [
    "#### VertLENS2StatsLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b8ce74a-c0c8-4132-8b74-ca923cba7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VertLENS2StatsLoop(ds_plens, time_type, varname):\n",
    "    ds_plens = ds_plens.groupby('plev')\n",
    "    \n",
    "    lev_stats_list = []\n",
    "    \n",
    "    for lev, dlev in ds_plens:\n",
    "        path = path_to_plotdata+'Map.Trend.LENSstats.'+varname+'.'+sd_str+'.Z'+str(int(lev))+'.'+td_str+'.'+time_outstr+'.'+ens_str+'.nc'\n",
    "        if os.path.isfile(path):\n",
    "            print('Loading LENS2 piControl trend distribution for Z'+str(int(lev)))\n",
    "            dlev_stats = xr.open_dataset(path)\n",
    "        else:\n",
    "            print('Calculating spatial trends for '+str(lev)+' hPa')\n",
    "            dlev_stats = LENS2TrendsEnsemble('Map.Trend',dlev, time_type, varname, True)\n",
    "            SaveData(dlev_stats, graph_type_str+'.LENSstats', varname, time_outstr, lev)\n",
    "            \n",
    "        lev_stats_list.append(dlev_stats)\n",
    "\n",
    "    ds_plens_stats = xr.concat(lev_stats_list, dim='plev')\n",
    "    \n",
    "    return ds_plens_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33455cec-cbe3-48ad-982e-7095cf42869d",
   "metadata": {},
   "source": [
    "#### LENS2TrendsEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1963d6c2-56a3-462c-aa45-4249231b9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LENS2TrendsEnsemble(graph_type, ds_lens, time_type, varname, loop):\n",
    "    path = path_to_plotdata+graph_type+'.LENSstats.'+varname+'.'+sd_str+'.'+td_str+'.'+time_outstr+'.'+ens_str+'.nc'\n",
    "    if os.path.isfile(path) and not loop:\n",
    "        print('Loading LENS2 piControl trend distribution')\n",
    "        ds_ens_stats = xr.open_dataset(path)\n",
    "    \n",
    "    else:\n",
    "        print('Calculating LENS2 piControl trend distribution')\n",
    "        # Calculating averages\n",
    "        if time_avg == 0:\n",
    "            ds_lens = ds_lens.groupby('time.month')\n",
    "            time_list = np.arange(1,13)\n",
    "            \n",
    "        # Seasonal averaging\n",
    "        elif time_avg == 2:\n",
    "            ds_lens = ds_lens.resample(time='QS-DEC').mean('time')\n",
    "            ds_lens = ds_lens.groupby('time.month')\n",
    "            time_list = np.arange(3,13,3)\n",
    "        \n",
    "        # Calculate trends\n",
    "        trends_avg_list = []\n",
    "        trends_std_list = []\n",
    "    \n",
    "         # Cycle through months/seasons\n",
    "        for m, ds in ds_lens:\n",
    "            print('Spatial trends for '+str(m))\n",
    "    \n",
    "            # Add linear time coordinate for trends - doesn't work with datetime time coordinate\n",
    "            ds = AddCoordTrend(ds, time_type)\n",
    "    \n",
    "            print('  Calculating trends for LENS2 piControl')\n",
    "            ds_gp_trend = CalcTrend(ds.load(), time_type)\n",
    "            ds_mean = ds_gp_trend.mean('slice', skipna = True)\n",
    "            ds_std = ds_gp_trend.std('slice', skipna = True)          \n",
    "                \n",
    "            trends_avg_list.append(ds_mean.rename('LENS2 piControl mu'))\n",
    "            trends_std_list.append(ds_std.rename('LENS2 piControl sigma'))\n",
    "    \n",
    "        ds_ens_mu = xr.concat(trends_avg_list, dim=pd.Index(time_list, name=time_outstr))\n",
    "        ds_ens_sigma = xr.concat(trends_std_list, dim=pd.Index(time_list, name=time_outstr))\n",
    "        ds_ens_stats = xr.merge([ds_ens_mu, ds_ens_sigma])\n",
    "\n",
    "        if not loop:\n",
    "            SaveData(ds_ens_stats, graph_type_str+'.LENSstats', varname, time_outstr)\n",
    "            \n",
    "\n",
    "    return ds_ens_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851104ea-5378-464b-90a9-70d329a3228d",
   "metadata": {},
   "source": [
    "#### VertSptrendsLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2f1077c-8464-4a6b-a4ff-2299756147f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VertSptrendsLoop(ds_sptpl, ds_plens_ens, time_type, varname):\n",
    "    ds_sptpl = ds_sptpl.groupby('plev')\n",
    "    ds_plens_ens = ds_plens_ens.groupby('plev')\n",
    "    \n",
    "    lev_spt_list = []\n",
    "    lev_pval_list = []\n",
    "    for lev, dsp_lev in ds_sptpl:\n",
    "        dlen_lev = ds_plens_ens[lev]\n",
    "        print('Calculating spatial trends for '+str(lev)+' hPa')\n",
    "        dlev_trends, dlev_pval = SpatZonTrends(dsp_lev, dlen_lev, time_type, varname)\n",
    "        lev_spt_list.append(dlev_trends)\n",
    "        lev_pval_list.append(dlev_pval)\n",
    "        \n",
    "    ds_pltrends = xr.concat(lev_spt_list, dim='plev')\n",
    "    ds_plpval = xr.concat(lev_pval_list, dim='plev')\n",
    "\n",
    "    return ds_pltrends, ds_plpval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600fc30-8a53-42a1-8daf-8cd675880757",
   "metadata": {},
   "source": [
    "#### SpatZonTrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3aa1a1d-7132-407b-9d13-91130c354f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpatZonTrends(ds, ds_lens_ens, time_type, varname):\n",
    "    ds_spz = ds.drop_vars(['LENS2 piControl'])\n",
    "    ds_spz = ds_spz.drop_dims('slice')\n",
    "\n",
    "    # Bool for spatial/zonal differentiation\n",
    "    dimlist = ds_spz['ERA5'].dims\n",
    "    zonspt_bool = 'lonE' in dimlist # True: spatial, False: zonal\n",
    "    \n",
    "    # Varname bool\n",
    "    var_bool = varname == 'Z3'\n",
    "    \n",
    "    # Calculating averages\n",
    "    if time_avg == 0:\n",
    "        ds_spz = ds_spz.groupby('time.month')\n",
    "        \n",
    "    # Seasonal averaging\n",
    "    elif time_avg == 2:\n",
    "        ds_spz = ds_spz.resample(time='QS-DEC').mean('time')\n",
    "        ds_spz = ds_spz.groupby('time.month')\n",
    "    \n",
    "    # Calculate trends\n",
    "    trends_tm_list = []\n",
    "    pvals_tm_list = []\n",
    "    \n",
    "    # Cycle through months/seasons\n",
    "    for m, ds in ds_spz:\n",
    "        \n",
    "        print('Trends for '+str(m))\n",
    "        trend_ds_list = []\n",
    "        pval_ds_list = []\n",
    "\n",
    "        # Add linear time coordinate for trends - doesn't work with datetime time coordinate\n",
    "        ds = AddCoordTrend(ds, time_type)\n",
    "\n",
    "        # Index LENS stats\n",
    "        ds_stats = ds_lens_ens.loc[{time_outstr: m}]\n",
    "\n",
    "        # Cycle through all variables\n",
    "        for dsname, da in ds.items():\n",
    "            print('  Calculating trends for '+dsname)\n",
    "            da_gp_trend = CalcTrend(da.load(), time_type) \n",
    "            \n",
    "            trend_ds_list.append(da_gp_trend.rename(dsname))\n",
    "\n",
    "            era_bool = dsname == 'ERA5'\n",
    "\n",
    "            # Calculate p-val and pcrit against LENS2\n",
    "            # If LENS2 ensemble mean or ERA5 & 2D variable, skip p-value calculation\n",
    "            if dsname != 'LENS2 piControl mean' and (not era_bool or ((not file_bool and zonspt_bool) or (not var_bool and not zonspt_bool))):\n",
    "                # If 3D variable & ERA5, regrid data to CESM2\n",
    "                if era_bool and zonspt_bool:\n",
    "                    # If Z3 (i.e. not on CESM2 grid)\n",
    "                    if var_bool:\n",
    "                        da_gp_trend = Regrid(da_gp_trend, regridderERA, 'era')\n",
    "                    else:\n",
    "                        da_gp_trend = da_gp_trend.rename({'latE': 'lat', 'lonE': 'lon'})\n",
    "                # If ERA5 & zonal (and thus not including Z3)\n",
    "                elif era_bool and not zonspt_bool:\n",
    "                    da_gp_trend = da_gp_trend.rename({'latE': 'lat'})\n",
    "                    \n",
    "                # Calculate p-value and p-critical value\n",
    "                print('  Calculating p-values and p-critical values for '+dsname)\n",
    "                da_pval = CalcSlopeSig(ds_stats, da_gp_trend)\n",
    "                \n",
    "                # Rename\n",
    "                da_pval = da_pval.rename(dsname)\n",
    "                da_pval = da_pval.to_dataset()\n",
    "                da_pval = da_pval.assign({dsname+' pcrit': da_pval['pcrit']})\n",
    "                da_pval = da_pval.drop_vars('pcrit')\n",
    "                pval_ds_list.append(da_pval)\n",
    "            \n",
    "        ds_trend_slice = xr.merge(trend_ds_list, compat='no_conflicts')\n",
    "        trends_tm_list.append(ds_trend_slice)\n",
    "        \n",
    "        ds_pval_slice = xr.merge(pval_ds_list, compat='no_conflicts')\n",
    "        pvals_tm_list.append(ds_pval_slice)\n",
    "\n",
    "    ds_trend = xr.concat(trends_tm_list, dim=pd.Index(date_str, name=time_outstr))\n",
    "    ds_pval = xr.concat(pvals_tm_list, dim=pd.Index(date_str, name=time_outstr))\n",
    "\n",
    "    return ds_trend, ds_pval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b8c28-e1e6-48ff-92f5-d237903488be",
   "metadata": {},
   "source": [
    "#### CalcSlopeSig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ac30338-2fbb-4bd8-9b90-5af455db2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcSlopeSig(ds_lens, da):\n",
    "    # Extract statistics\n",
    "    mu_lens = ds_lens['LENS2 piControl mu']\n",
    "    sig_lens = ds_lens['LENS2 piControl sigma']\n",
    "    \n",
    "    # Z statistic\n",
    "    zstat = (da-mu_lens)/sig_lens\n",
    "\n",
    "    # Calculate p-value\n",
    "    pval = stats.norm.sf(abs(zstat))*2\n",
    "\n",
    "    # Add p-val to DataArray with new data\n",
    "    da_pval = da.copy(data=pval)\n",
    "\n",
    "    # Calculate p_critical\n",
    "    pcrit = Wilks_pcrit(pval, 0.05)\n",
    "\n",
    "    da_pval['pcrit'] = pcrit\n",
    "    print(pcrit)\n",
    "    \n",
    "    return da_pval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749eee63-dae5-4712-aeb4-75336e8e72fe",
   "metadata": {},
   "source": [
    "#### PattCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d86ee75a-d2c9-4316-86ba-97cf3588e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PattCorr(ds_trend):\n",
    "    ## Calculation pattern correlation coefficient\n",
    "    ds_trend = ds_trend.groupby(time_outstr)\n",
    "\n",
    "    # Calculate pattern correlation\n",
    "    pcorr_m_list = []\n",
    "\n",
    "    for m, ds in ds_trend:\n",
    "        print('Pattern correlation for '+str(m))\n",
    "        pcorr_ds_list = []\n",
    "\n",
    "        # Pull out ERA5\n",
    "        da_era = ds['ERA5']\n",
    "\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "\n",
    "        for dsname, da in ds.items():\n",
    "            if dsname != 'ERA5':\n",
    "                # Calculate corr\n",
    "                da_pcorr = xr.corr(da_era, da, weights=weights)\n",
    "\n",
    "                pcorr_ds_list.append(da_pcorr.rename(dsname))\n",
    "\n",
    "        ds_pcorr_slice = xr.merge(pcorr_ds_list)\n",
    "        pcorr_m_list.append(ds_pcorr_slice)\n",
    "\n",
    "\n",
    "    ds_pcorr = xr.concat(pcorr_m_list, dim=pd.Index(date_str, name=time_outstr))\n",
    "    return ds_pcorr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a812b03-1596-4b44-b8b8-c0797b2110eb",
   "metadata": {},
   "source": [
    "#### SpatTrendRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9ee18a2-17fe-46d4-af2d-d208c105b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SpatTrendRatio(ds_trend, rat_type):\n",
    "#     ## warm: contribution to obs warming trends only\n",
    "#     ## cold: contribution to obs cooling trends only\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7c0aa-0932-42a1-97e2-e17c3c1a5853",
   "metadata": {},
   "source": [
    "#### SpatZonAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0645dc25-d32d-4891-b121-262f6cbc9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpatZonAvg(ds):\n",
    "    # Calculating averages\n",
    "    if time_avg == 0:\n",
    "        ds_avg = ds.groupby('time.month').mean('time')\n",
    "        \n",
    "    # Seasonal averaging\n",
    "    elif time_avg == 2:\n",
    "        ds_avg = ds.resample(time='QS-DEC').mean('time')\n",
    "        ds_avg = ds_avg.groupby('time.month').mean('time')\n",
    "        ds_avg = ds_avg.assign_coords(month=seas_str)\n",
    "        ds_avg = ds_avg.rename({'month':'season'})\n",
    "\n",
    "    return ds_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a293c-fd19-4a00-8909-9819003295df",
   "metadata": {},
   "source": [
    "#### SpatialVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9951e7e1-46f7-4ce6-8754-b588ecb61c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpatialVar(ds):\n",
    "    # Calculating variances\n",
    "    if time_avg == 0:\n",
    "        ds_var = ds.groupby('time.month').var('time')\n",
    "        \n",
    "    # Seasonal variances\n",
    "    elif time_avg == 2:\n",
    "        ds_avg = ds.resample(time='QS-DEC').mean('time')\n",
    "        ds_var = ds_avg.groupby('time.month').var('time')\n",
    "        ds_var = ds_var.assign_coords(month=seas_str)\n",
    "        ds_var = ds_var.rename({'month':'season'})\n",
    "\n",
    "    return ds_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e7baa-fcac-4c8c-b24d-86f0188fa8a1",
   "metadata": {},
   "source": [
    "#### AddAllCyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff4fe468-dc38-49c8-b78b-7bc53e451956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddAllCyclic(ds_trend):\n",
    "    londim = defaultdict(lambda: 'lon')\n",
    "    londim['ERA5'] = 'lonE'\n",
    "\n",
    "    latdim = defaultdict(lambda: 'lat')\n",
    "    latdim['ERA5'] = 'latE'\n",
    "    \n",
    "    cyclic_ds_list = []\n",
    "        \n",
    "    for dsname, da in ds_trend.items():\n",
    "        if 'pcrit' in dsname:\n",
    "            cyclic_ds_list.append(da.rename(dsname))\n",
    "        else:\n",
    "            if file_bool:\n",
    "                da = da.transpose(time_outstr, latdim[dsname],londim[dsname])\n",
    "            else:\n",
    "                da = da.transpose(time_outstr, 'plev',latdim[dsname],londim[dsname])\n",
    "            da_cyc = AddCyclic(da, londim[dsname])\n",
    "            \n",
    "            cyclic_ds_list.append(da_cyc.rename(dsname))\n",
    "            \n",
    "    ds_trend = xr.merge(cyclic_ds_list)\n",
    "\n",
    "    return ds_trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05316d5e-f13b-4cc7-ab51-0dedfb47392f",
   "metadata": {},
   "source": [
    "#### SaveData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b86e86b4-6ec9-4718-b32a-f4d0e6abe5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveData(ds, plot_type, varname, tavg, plot_level=None):\n",
    "    level_str = '' if plot_level == None else 'Z'+str(int(plot_level))+'.'\n",
    "    \n",
    "    filename = plot_type+'.'+varname+'.'+sd_str+'.'+level_str+td_str+'.'+tavg+'.'+ens_str+'.nc'\n",
    "    \n",
    "    print('Saving '+filename)\n",
    "    # File format is:\n",
    "    # (plot type, including special averaging, i.e. anomalies).varname.spatialdomain.timedomain.timeaveraging.ensemble_type.nc\n",
    "    ds.to_netcdf(path_to_plotdata+filename,\n",
    "                format='NETCDF4')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8dfd0d-3e9b-4e7a-a1d4-f6c77aee3914",
   "metadata": {},
   "source": [
    "### Process and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f060146-ba1c-422b-a811-964274ede89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure out processing calculations to do\n",
      "LENS2 piControl\n",
      "  Initial data loading complete\n",
      "  Fixed time dimension\n",
      "  Sliced data\n",
      "  No proceesing, spatial\n",
      "  No proceesing, spatial\n",
      "  Processing on dataset complete\n",
      "PiC_UVnudge_2006\n",
      "  Initial data loading complete\n",
      "  Fixed time dimension\n",
      "  Sliced data\n",
      "  No proceesing, spatial\n",
      "  No proceesing, spatial\n",
      "  Calculating ensemble mean\n",
      "  Processing on dataset complete\n",
      "PiC_UVnudge_LM2006\n",
      "  Initial data loading complete\n",
      "  Fixed time dimension\n",
      "  Sliced data\n",
      "  No proceesing, spatial\n",
      "  No proceesing, spatial\n",
      "  Calculating ensemble mean\n",
      "  Processing on dataset complete\n",
      "PiC_UVnudge_MM2006\n",
      "  Initial data loading complete\n",
      "  Fixed time dimension\n",
      "  Sliced data\n",
      "  No proceesing, spatial\n",
      "  No proceesing, spatial\n",
      "  Calculating ensemble mean\n",
      "  Processing on dataset complete\n",
      "ERA5\n",
      "  Initial data loading complete\n",
      "  Fixed time dimension\n",
      "  Sliced data\n",
      "  No proceesing, spatial\n",
      "  No proceesing, spatial\n",
      "  Processing on dataset complete\n",
      "All datasets merged\n",
      "CPU times: user 7.09 s, sys: 916 ms, total: 8.01 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ds_proc = CreateMasterDS(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a88f03c5-3103-4155-8ee0-d4fca73b3f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add U & V variables if doing spatial trends or patterns and original variable is geopotential\n",
    "if (plot_types['zonal'][0] or plot_types['spatial'][0]) and var == 'Z3':\n",
    "    # Create U & V datasets\n",
    "    ds_u = CreateMasterDS('U')\n",
    "    ds_v = CreateMasterDS('V')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417424a-da41-463c-9ddf-9739a411c8f2",
   "metadata": {},
   "source": [
    "### Plotting set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d8dc3eb-9780-40b0-bb80-fbd06fdbe74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 0 ns, total: 17 µs\n",
      "Wall time: 18.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mon_str = np.array(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "seas_str = np.array(['MAM','JJA','SON','DJF'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67cf387-825b-48f8-966d-c0193dcf1952",
   "metadata": {},
   "source": [
    "## Month trend plots\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac6b8c03-b495-42d7-a279-da855ddd79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_types['mtrd'][0]:\n",
    "    graph_type_str = 'Linear.Trend'\n",
    "    ## Select plot type - timeseries or monthly - to make and assign variables accordings\n",
    "    \n",
    "\n",
    "    # Timeseries\n",
    "    if time_avg == 4:\n",
    "        dim_avg = 'time.month'\n",
    "        period = 'month'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbca67-dc02-49a6-88d9-1593aac639aa",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e83bf9b-a754-4fc9-bce3-45b7872b879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plot_types['mtrd'][0]:\n",
    "\n",
    "    # Calculate monthly trends\n",
    "    mon_trd_list = []\n",
    "    for dsname, da in ds_proc.items():\n",
    "        da_mon_trd = CalcMonthTrd(da)\n",
    "        mon_trd_list.append(da_mon_trd.rename(dsname))\n",
    "\n",
    "    ds_mon_trd = xr.merge(mon_trd_list)\n",
    "    ds_mon_trd['LENS2 piControl min'] = ds_mon_trd['LENS2 piControl'].min('slice')\n",
    "    ds_mon_trd['LENS2 piControl max'] = ds_mon_trd['LENS2 piControl'].max('slice')\n",
    "\n",
    "    # Calculate annual trends\n",
    "    ann_trd_list = []\n",
    "    for dsname, da in ds_proc.items():\n",
    "        da_ann_trd = CalcAnnTrd(da)\n",
    "        ann_trd_list.append(da_ann_trd.rename(dsname))\n",
    "\n",
    "    ds_ann_trd = xr.merge(ann_trd_list)\n",
    "    ds_ann_trd['LENS2 piControl min'] = ds_ann_trd['LENS2 piControl'].min('slice')\n",
    "    ds_ann_trd['LENS2 piControl max'] = ds_ann_trd['LENS2 piControl'].max('slice')\n",
    "\n",
    "    # Write out data\n",
    "    SaveData(ds_mon_trd, graph_type_str, var, 'month')\n",
    "    SaveData(ds_ann_trd, graph_type_str, var, 'year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797221f-ed65-4e23-a96d-9c7cad05e8cf",
   "metadata": {},
   "source": [
    "## Line plots\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a2d71d2-7dce-4b60-be14-77c7a8b33971",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_types['line'][0]:\n",
    "    graph_type_str = 'Linear'\n",
    "        \n",
    "\n",
    "    # Time averaging for yearly plot\n",
    "    if time_avg == 1:\n",
    "        dim_avg = 'time.year'  \n",
    "        period='year'\n",
    "    if time_avg == 4:\n",
    "        dim_avg = 'time.month'\n",
    "        period ='time'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5be72b-e7ff-4635-ac88-bd6adaae97e2",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a06a0a0-cf51-4a57-88e3-c6e481f7599a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plot_types['line'][0]:\n",
    "\n",
    "    ## Absolute (only one that will be used for SIA-one month, TOA, AMOC)\n",
    "    # Yearly averaging for absolute\n",
    "    if time_avg == 1:\n",
    "        ds_abs = ds_proc.groupby(dim_avg).mean('time')\n",
    "\n",
    "        ds_abs['LENS2 piControl min'] = ds_abs['LENS2 piControl'].min('slice')\n",
    "        ds_abs['LENS2 piControl max'] = ds_abs['LENS2 piControl'].max('slice')\n",
    "        \n",
    "        SaveData(ds_abs, graph_type_str+'.abs', var, period)\n",
    "\n",
    "    elif time_avg == 4:\n",
    "        # If only plotting one month from SIA data\n",
    "        ds_abs = ds_proc.groupby(dim_avg)\n",
    "\n",
    "        for m, ds in ds_abs:\n",
    "            ds['LENS2 piControl min'] = ds['LENS2 piControl'].min('slice')\n",
    "            ds['LENS2 piControl max'] = ds['LENS2 piControl'].max('slice')\n",
    "\n",
    "            SaveData(ds, graph_type_str+'.abs', var, mon_str[m-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a80e7a6b-b0f1-4c40-bfe6-a355e3d981f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plot_types['line'][0] and t_domain == 1950:\n",
    "    # Variables in terms of how many years they've been nudged\n",
    "    if time_avg == 1:\n",
    "        ds_pic = DropNonPiC(ds_abs)\n",
    "        ds_nudyr = NudgeYears(ds_pic, period)\n",
    "\n",
    "        SaveData(ds_nudyr, graph_type_str+'.nudyr', var, period)\n",
    "\n",
    "    elif time_avg == 4:\n",
    "        # If only plotting one month from SIA data\n",
    "        if plots['sia'][0]:\n",
    "            for m, ds in ds_abs:\n",
    "                ds = DropNonPiC(ds)\n",
    "\n",
    "                ds_nudyr = NudgeYears(ds, period)\n",
    "                SaveData(ds_nudyr, graph_type_str+'.nudyr', var, mon_str[m-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61d38580-27cf-4a9c-9bec-2b0e94e2cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plot_types['line'][0]:\n",
    "    # If sea ice area, make absolute correlation coefficients\n",
    "    if time_avg == 4:\n",
    "        # Calculate R for each month\n",
    "        for m, ds in ds_abs:\n",
    "            abs_r_list = []\n",
    "            era5_abs = ds['ERA5']\n",
    "            for dsname, attrs in ds_names.items():\n",
    "                if attrs[0] and (attrs[1] == 1 or attrs[1] == 2):\n",
    "                    da_abs_r = CalcR(era5_abs, ds[dsname], period, attrs[1])\n",
    "                    abs_r_list.append(da_abs_r.rename(dsname))\n",
    "                    \n",
    "            ds_abs_r = xr.merge(abs_r_list)\n",
    "            SaveData(ds_abs_r, graph_type_str+'.absR', var, mon_str[m-1])\n",
    "\n",
    "\n",
    "        # Anomalies\n",
    "        anom_list = []\n",
    "        for m, ds in ds_abs:\n",
    "            da_anom = CalcAnom(ds, period)\n",
    "            da_anom['LENS2 piControl min'] = da_anom['LENS2 piControl'].min('slice')\n",
    "            da_anom['LENS2 piControl max'] = da_anom['LENS2 piControl'].max('slice')\n",
    "            \n",
    "            anom_list.append(da_anom)\n",
    "            SaveData(da_anom, graph_type_str+'.anom', var, mon_str[m-1])\n",
    "            \n",
    "        ds_anom = xr.concat(anom_list, dim='time')\n",
    "\n",
    "        # Calculate R for anomalies\n",
    "        ds_anom = ds_anom.groupby(dim_avg)\n",
    "        for m, ds in ds_anom:\n",
    "            anom_r_list = []\n",
    "            era5_anom = ds['ERA5']\n",
    "            for dsname, attrs in ds_names.items():\n",
    "                if attrs[0] and (attrs[1] == 1 or attrs[1] == 2):\n",
    "                    da_anom_r = CalcR(era5_anom, ds[dsname], period, attrs[1])\n",
    "                    anom_r_list.append(da_anom_r.rename(dsname))\n",
    "                    \n",
    "            ds_anom_r = xr.merge(anom_r_list)\n",
    "            SaveData(ds_anom_r, graph_type_str+'.anomR', var, mon_str[m-1])\n",
    "\n",
    "        # Detrended anomalies\n",
    "        dtrd_anom_list = []\n",
    "        for m, ds in ds_anom:\n",
    "            dtrd_m_list = []\n",
    "            for dsname, da in ds.items():\n",
    "                da_dtrd = CalcDetAnom(da, period)\n",
    "                dtrd_m_list.append(da_dtrd)\n",
    "                \n",
    "            da_dtrd = xr.merge(dtrd_m_list)\n",
    "            da_dtrd['LENS2 piControl min'] = da_dtrd['LENS2 piControl'].min('slice')\n",
    "            da_dtrd['LENS2 piControl max'] = da_dtrd['LENS2 piControl'].max('slice')\n",
    "            \n",
    "            dtrd_anom_list.append(da_dtrd)\n",
    "            SaveData(da_dtrd, graph_type_str+'.anomdtrd', var, mon_str[m-1])\n",
    "            \n",
    "        ds_dtrd = xr.concat(dtrd_anom_list, dim='time')\n",
    "\n",
    "        # Calculate R for detrended anomalies\n",
    "        ds_dtrd = ds_dtrd.groupby(dim_avg)\n",
    "        for m, ds in ds_dtrd:\n",
    "            dtrd_r_list = []\n",
    "            era5_dtrd = ds['ERA5']\n",
    "            for dsname, attrs in ds_names.items():\n",
    "                if attrs[0] and (attrs[1] == 1 or attrs[1] == 2):\n",
    "                    da_dtrd_r = CalcR(era5_dtrd, ds[dsname], period, attrs[1])\n",
    "                    dtrd_r_list.append(da_dtrd_r.rename(dsname))\n",
    "                    \n",
    "            ds_dtrd_r = xr.merge(dtrd_r_list)\n",
    "            SaveData(ds_dtrd_r, graph_type_str+'.anomdtrdR', var, mon_str[m-1])\n",
    "    \n",
    "    \n",
    "    # If surface temperature, make anomaly, detrended anomaly (yearly only), and ensemble spread plots\n",
    "    elif plots['ts'][0]:\n",
    "        # Calculate R for absolute temperature\n",
    "        abs_r_list = []\n",
    "        era5_abs = ds_abs['ERA5']\n",
    "        for dsname, attrs in ds_names.items():\n",
    "            if attrs[0] and (attrs[1] == 1 or attrs[1] == 2):\n",
    "                da_abs_r = CalcR(era5_abs, ds_abs[dsname], period, attrs[1])\n",
    "                abs_r_list.append(da_abs_r.rename(dsname))\n",
    "                \n",
    "        ds_abs_r = xr.merge(abs_r_list)\n",
    "        SaveData(ds_abs_r, graph_type_str+'.absR', var, period)\n",
    "        \n",
    "        # Anomalies\n",
    "        ds_anom = CalcAnom(ds_abs, period)\n",
    "        ds_anom['LENS2 piControl min'] = ds_anom['LENS2 piControl'].min('slice')\n",
    "        ds_anom['LENS2 piControl max'] = ds_anom['LENS2 piControl'].max('slice')\n",
    "        \n",
    "        SaveData(ds_anom, graph_type_str+'.anom', var, period)\n",
    "\n",
    "        # Calculate R for anomalies\n",
    "        anom_r_list = []\n",
    "        era5_anom = ds_anom['ERA5']\n",
    "        for dsname, attrs in ds_names.items():\n",
    "            if attrs[0] and (attrs[1] == 1 or attrs[1] == 2):\n",
    "                da_anom_r = CalcR(era5_anom, ds_anom[dsname], period, attrs[1])\n",
    "                anom_r_list.append(da_anom_r.rename(dsname))\n",
    "                \n",
    "        ds_anom_r = xr.merge(anom_r_list)\n",
    "        SaveData(ds_anom_r, graph_type_str+'.anomR', var, period)\n",
    "\n",
    "        # Ensemble spread\n",
    "        enspd_list = []\n",
    "        for dsname, attrs in ds_names.items():\n",
    "            if attrs[0] and attrs[1] == 1:\n",
    "                da_enspd = CalcEnsSp(ds_abs[dsname])\n",
    "                enspd_list.append(da_enspd.rename(dsname))\n",
    "                \n",
    "        ds_enspd = xr.merge(enspd_list)\n",
    "        SaveData(ds_enspd, graph_type_str+'.espd', var, period)\n",
    "\n",
    "        # Detrended anomalies only if yearly\n",
    "        # Detrended anomalies\n",
    "        dtrd_anom_list = []\n",
    "        for dsname, da in ds_anom.items():\n",
    "            da_dtrd = CalcDetAnom(da, period)\n",
    "            dtrd_anom_list.append(da_dtrd.rename(dsname))\n",
    "\n",
    "        ds_dtrd = xr.merge(dtrd_anom_list)\n",
    "        ds_dtrd['LENS2 piControl min'] = ds_dtrd['LENS2 piControl'].min('slice')\n",
    "        ds_dtrd['LENS2 piControl max'] = ds_dtrd['LENS2 piControl'].max('slice')\n",
    "\n",
    "        SaveData(ds_dtrd, graph_type_str+'.anomdtrd', var, period)\n",
    "\n",
    "        # Calculate R for detrended anomalies\n",
    "        dtrd_r_list = []\n",
    "        era5_dtrd = ds_dtrd['ERA5']\n",
    "        for dsname, attrs in ds_names.items():\n",
    "            if attrs[0] and (attrs[1] == 1 or attrs[1] == 2):\n",
    "                da_dtrd_r = CalcR(era5_dtrd, ds_dtrd[dsname], period, attrs[1])\n",
    "                dtrd_r_list.append(da_dtrd_r.rename(dsname))\n",
    "\n",
    "        ds_dtrd_r = xr.merge(dtrd_r_list)\n",
    "        SaveData(ds_dtrd_r, graph_type_str+'.anomdtrdR', var, period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5bd11-9c4c-4fdb-a759-ffb7d15f293a",
   "metadata": {},
   "source": [
    "## Create Regridders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18094d5a-0b41-4f6c-a6c4-945fc958f218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.25 s, sys: 106 ms, total: 2.36 s\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if plot_types['spatial'][0]:\n",
    "    ## Only run after time averaging!!!\n",
    "    lats = np.array([50.4188481675393, 51.3612565445026, 52.303664921466, \n",
    "        53.2460732984293, 54.1884816753927, 55.130890052356, 56.0732984293194, \n",
    "        57.0157068062827, 57.9581151832461, 58.9005235602094, 59.8429319371728, \n",
    "        60.7853403141361, 61.7277486910995, 62.6701570680628, 63.6125654450262, \n",
    "        64.5549738219895, 65.4973821989529, 66.4397905759162, 67.3821989528796, \n",
    "        68.3246073298429, 69.2670157068063, 70.2094240837696, 71.151832460733, \n",
    "        72.0942408376963, 73.0366492146597, 73.979057591623, 74.9214659685864, \n",
    "        75.8638743455497, 76.8062827225131, 77.7486910994764, 78.6910994764398, \n",
    "        79.6335078534031, 80.5759162303665, 81.5183246073298, 82.4607329842932, \n",
    "        83.4031413612565, 84.3455497382199, 85.2879581151832, 86.2303664921466, \n",
    "        87.17277486911, 88.1151832460733, 89.0575916230366, 90])\n",
    "    lons = np.array([-180, -178.75, -177.5, -176.25, -175, -173.75, -172.5, -171.25, -170, \n",
    "        -168.75, -167.5, -166.25, -165, -163.75, -162.5, -161.25, -160, -158.75, \n",
    "        -157.5, -156.25, -155, -153.75, -152.5, -151.25, -150, -148.75, -147.5, \n",
    "        -146.25, -145, -143.75, -142.5, -141.25, -140, -138.75, -137.5, -136.25, \n",
    "        -135, -133.75, -132.5, -131.25, -130, -128.75, -127.5, -126.25, -125, \n",
    "        -123.75, -122.5, -121.25, -120, -118.75, -117.5, -116.25, -115, -113.75, \n",
    "        -112.5, -111.25, -110, -108.75, -107.5, -106.25, -105, -103.75, -102.5, \n",
    "        -101.25, -100, -98.75, -97.5, -96.25, -95, -93.75, -92.5, -91.25, -90, \n",
    "        -88.75, -87.5, -86.25, -85, -83.75, -82.5, -81.25, -80, -78.75, -77.5, \n",
    "        -76.25, -75, -73.75, -72.5, -71.25, -70, -68.75, -67.5, -66.25, -65, \n",
    "        -63.75, -62.5, -61.25, -60, -58.75, -57.5, -56.25, -55, -53.75, -52.5, \n",
    "        -51.25, -50, -48.75, -47.5, -46.25, -45, -43.75, -42.5, -41.25, -40, \n",
    "        -38.75, -37.5, -36.25, -35, -33.75, -32.5, -31.25, -30, -28.75, -27.5, \n",
    "        -26.25, -25, -23.75, -22.5, -21.25, -20, -18.75, -17.5, -16.25, -15, \n",
    "        -13.75, -12.5, -11.25, -10, -8.75, -7.5, -6.25, -5, -3.75, -2.5, -1.25, \n",
    "        0, 1.25, 2.5, 3.75, 5, 6.25, 7.5, 8.75, 10, 11.25, 12.5, 13.75, 15, \n",
    "        16.25, 17.5, 18.75, 20, 21.25, 22.5, 23.75, 25, 26.25, 27.5, 28.75, 30, \n",
    "        31.25, 32.5, 33.75, 35, 36.25, 37.5, 38.75, 40, 41.25, 42.5, 43.75, 45, \n",
    "        46.25, 47.5, 48.75, 50, 51.25, 52.5, 53.75, 55, 56.25, 57.5, 58.75, 60, \n",
    "        61.25, 62.5, 63.75, 65, 66.25, 67.5, 68.75, 70, 71.25, 72.5, 73.75, 75, \n",
    "        76.25, 77.5, 78.75, 80, 81.25, 82.5, 83.75, 85, 86.25, 87.5, 88.75, 90, \n",
    "        91.25, 92.5, 93.75, 95, 96.25, 97.5, 98.75, 100, 101.25, 102.5, 103.75, \n",
    "        105, 106.25, 107.5, 108.75, 110, 111.25, 112.5, 113.75, 115, 116.25, \n",
    "        117.5, 118.75, 120, 121.25, 122.5, 123.75, 125, 126.25, 127.5, 128.75, \n",
    "        130, 131.25, 132.5, 133.75, 135, 136.25, 137.5, 138.75, 140, 141.25, \n",
    "        142.5, 143.75, 145, 146.25, 147.5, 148.75, 150, 151.25, 152.5, 153.75, \n",
    "        155, 156.25, 157.5, 158.75, 160, 161.25, 162.5, 163.75, 165, 166.25, \n",
    "        167.5, 168.75, 170, 171.25, 172.5, 173.75, 175, 176.25, 177.5, 178.75])\n",
    "    \n",
    "    # NEITHER REGRIDDER INCLUDES CYCLIC POINT!\n",
    "    \n",
    "    if var == 'aice':\n",
    "        ## Create Regridder for CICE grid\n",
    "        lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "        \n",
    "        # Create target grid and sample nj ni grid \n",
    "        target_gridSIC = xr.Dataset({'lat': (['y', 'x'], lat2d),'lon': (['y', 'x'], lon2d)})\n",
    "        ds_samplens = ds_proc['LENS2 piControl mean'][0]\n",
    "        \n",
    "        # Create regridder for sea ice\n",
    "        regridderSIC = xe.Regridder(ds_samplens, target_gridSIC, 'nearest_s2d', reuse_weights=False)\n",
    "    \n",
    "    if plots['strd'][0]:\n",
    "    \n",
    "        # Create Regridder for ERA5->CESM2\n",
    "        target_gridERA = xr.Dataset({'lat': ('y', lats), 'lon': ('x', lons)})\n",
    "        ds_sampera = ds_proc['ERA5'][0]\n",
    "        ds_sampera = ds_sampera.rename({'latE':'lat', 'lonE': 'lon'})\n",
    "    \n",
    "        # Create regridder for ERA5\n",
    "        regridderERA = xe.Regridder(ds_sampera, target_gridERA, 'bilinear', reuse_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44e66ae3-c75c-46ed-bc03-5fe21ea0db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial plots\n",
    "if plot_types['spatial'][0]:\n",
    "    # Monthly\n",
    "    if time_avg == 0:\n",
    "        period = 'time'\n",
    "        date_str = mon_str\n",
    "\n",
    "    # Seasonally\n",
    "    elif time_avg == 2:\n",
    "        period = 'time'\n",
    "        date_str = seas_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920bc100-50d5-42c3-b698-82ad5e5cf26c",
   "metadata": {},
   "source": [
    "## Spatial trend plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1dd81-c0f2-4cd5-aa37-00f62be722c6",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1a16328-fa72-4bc6-aefa-cce958384432",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots['strd'][0]:\n",
    "    graph_type_str = 'Map.Trend'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8afd6e2-f546-4559-ba9a-85dd5fba61d1",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c31bcd7c-a945-4d51-bb32-e348584ac14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LENS2 piControl trend distribution\n",
      "Calculating spatial trends in aice\n",
      "Trends for 3\n",
      "  Calculating trends for PiC_UVnudge_2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_2006\n",
      "0.00014783770321974375\n",
      "  Calculating trends for PiC_UVnudge_LM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_LM2006\n",
      "0.00013909321403120852\n",
      "  Calculating trends for PiC_UVnudge_MM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_MM2006\n",
      "5.4586272872836186e-05\n",
      "  Calculating trends for ERA5\n",
      "  Calculating trends for LENS2 piControl mean\n",
      "Trends for 6\n",
      "  Calculating trends for PiC_UVnudge_2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_2006\n",
      "0.0002683124521338889\n",
      "  Calculating trends for PiC_UVnudge_LM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_LM2006\n",
      "0.000730232199461608\n",
      "  Calculating trends for PiC_UVnudge_MM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_MM2006\n",
      "5.880285704696379e-05\n",
      "  Calculating trends for ERA5\n",
      "  Calculating trends for LENS2 piControl mean\n",
      "Trends for 9\n",
      "  Calculating trends for PiC_UVnudge_2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_2006\n",
      "0.00010468925805039438\n",
      "  Calculating trends for PiC_UVnudge_LM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_LM2006\n",
      "0.0004035142855876803\n",
      "  Calculating trends for PiC_UVnudge_MM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_MM2006\n",
      "3.5373833121046618e-06\n",
      "  Calculating trends for ERA5\n",
      "  Calculating trends for LENS2 piControl mean\n",
      "Trends for 12\n",
      "  Calculating trends for PiC_UVnudge_2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_2006\n",
      "0.00020651069805541919\n",
      "  Calculating trends for PiC_UVnudge_LM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_LM2006\n",
      "0.000299348747853438\n",
      "  Calculating trends for PiC_UVnudge_MM2006\n",
      "  Calculating p-values and p-critical values for PiC_UVnudge_MM2006\n",
      "0.00011009894507721426\n",
      "  Calculating trends for ERA5\n",
      "  Calculating trends for LENS2 piControl mean\n",
      "CPU times: user 20min 32s, sys: 9.83 s, total: 20min 42s\n",
      "Wall time: 22min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['strd'][0]:\n",
    "    # If not 3D\n",
    "    if file_bool:\n",
    "        # Calculate statistics of piControl ensemble\n",
    "        ds_lens_stats = LENS2TrendsEnsemble(graph_type_str, ds_proc['LENS2 piControl'], period, var, False)\n",
    "        \n",
    "        # Calculate trends\n",
    "        print('Calculating spatial trends in '+var)\n",
    "        ds_sptrend, ds_sppval = SpatZonTrends(ds_proc, ds_lens_stats, period, var)\n",
    "\n",
    "    else:\n",
    "        ds_proc = ds_proc.where(ds_proc.plev.isin(plot_levels), drop=True)  \n",
    "        ds_u = ds_u.where(ds_u.plev.isin(plot_levels), drop=True)\n",
    "        ds_v = ds_v.where(ds_v.plev.isin(plot_levels), drop=True)\n",
    "        \n",
    "        # Calculate statistics for each level in the piControl ensemble\n",
    "        ds_lens_stats = VertLENS2StatsLoop(ds_proc['LENS2 piControl'], period, var)\n",
    "        ds_ulens_stats = VertLENS2StatsLoop(ds_u['LENS2 piControl'], period, 'U')\n",
    "        ds_vlens_stats = VertLENS2StatsLoop(ds_v['LENS2 piControl'], period, 'V')\n",
    "        \n",
    "        # Calculate trends\n",
    "        print('Calculating spatial trends in '+var)\n",
    "        ds_sptrend, ds_sppval = VertSptrendsLoop(ds_proc, ds_lens_stats, period, var)\n",
    "        print('Calculating spatial trends in U')\n",
    "        ds_usptrend, ds_usppval = VertSptrendsLoop(ds_u, ds_ulens_stats, period, 'U')\n",
    "        print('Calculating spatial trends in V')\n",
    "        ds_vsptrend, ds_vsppval = VertSptrendsLoop(ds_v, ds_vlens_stats, period, 'V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e5bf921-7dd4-4a43-9cc1-f3e16e881281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regridding CICE grid -> ATM grid...\n",
      "Regridding PiC_UVnudge_2006\n",
      "Regridding PiC_UVnudge_LM2006\n",
      "Regridding PiC_UVnudge_MM2006\n",
      "Regridding LENS2 piControl mean\n",
      "Regridding CICE grid -> ATM grid...\n",
      "Regridding PiC_UVnudge_2006\n",
      "Regridding PiC_UVnudge_LM2006\n",
      "Regridding PiC_UVnudge_MM2006\n",
      "Regridding ERA5 grid -> ATM grid...\n",
      "Regridding ERA5\n",
      "Pattern correlation for DJF\n",
      "Pattern correlation for JJA\n",
      "Pattern correlation for MAM\n",
      "Pattern correlation for SON\n",
      "Saving Map.Trend.pcorr.aice.Arctic.1980.season.Mean.nc\n",
      "CPU times: user 636 ms, sys: 35.9 ms, total: 672 ms\n",
      "Wall time: 711 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['strd'][0]:\n",
    "    if var == 'aice':\n",
    "        ds_sptrend = Regrid(ds_sptrend, regridderSIC, 'sic')\n",
    "        ds_sppval = Regrid(ds_sppval, regridderSIC, 'sic', True)\n",
    "        \n",
    "    # Regrid ERA5 data\n",
    "    ds_sptrend_corr = Regrid(ds_sptrend, regridderERA, 'era')\n",
    "\n",
    "    # Calculate pattern coefficients between ERA5-PiC_UVnudgeX\n",
    "    ds_trdcor = PattCorr(ds_sptrend_corr)\n",
    "\n",
    "    SaveData(ds_trdcor, graph_type_str+'.pcorr', var, time_outstr)\n",
    "\n",
    "    if var == 'Z3':\n",
    "        # Rename ERA5 lat/lon\n",
    "        ds_era = ds_usptrend['ERA5']\n",
    "        ds_era = ds_era.rename({'latE':'lat', 'lonE':'lon'})\n",
    "        ds_usptrend_corr = ds_usptrend.drop_vars('ERA5')\n",
    "        ds_usptrend_corr = ds_usptrend_corr.drop_dims(['latE','lonE'])\n",
    "        ds_usptrend_corr = ds_usptrend_corr.assign({'ERA5':ds_era})\n",
    "\n",
    "        ds_era = ds_vsptrend['ERA5']\n",
    "        ds_era = ds_era.rename({'latE':'lat', 'lonE':'lon'})\n",
    "        ds_vsptrend_corr = ds_vsptrend.drop_vars('ERA5')\n",
    "        ds_vsptrend_corr = ds_vsptrend_corr.drop_dims(['latE','lonE'])\n",
    "        ds_vsptrend_corr = ds_vsptrend_corr.assign({'ERA5':ds_era})\n",
    "        \n",
    "        # Calculate pattern coefficients between ERA5-PiC_UVnudgeX\n",
    "        ds_utrdcor = PattCorr(ds_usptrend_corr)\n",
    "        ds_vtrdcor = PattCorr(ds_vsptrend_corr)\n",
    "    \n",
    "        SaveData(ds_utrdcor, graph_type_str+'.pcorr', 'U', time_outstr)\n",
    "        SaveData(ds_vtrdcor, graph_type_str+'.pcorr', 'V', time_outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "667160ff-d1c9-49df-bef4-87fede05c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Map.Trend.aice.Arctic.1980.season.Mean.nc\n",
      "Saving Map.Trend.pval.aice.Arctic.1980.season.Mean.nc\n",
      "CPU times: user 57.1 ms, sys: 4.03 ms, total: 61.1 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['strd'][0]:\n",
    "    # Add cyclic data\n",
    "    ds_sptrend = AddAllCyclic(ds_sptrend)\n",
    "    ds_sppval = AddAllCyclic(ds_sppval)\n",
    "    \n",
    "    SaveData(ds_sptrend, graph_type_str, var, time_outstr)\n",
    "    SaveData(ds_sppval, graph_type_str+'.pval', var, time_outstr)\n",
    "\n",
    "    if var == 'Z3':\n",
    "        ds_usptrend = AddAllCyclic(ds_usptrend)\n",
    "        ds_usppval = AddAllCyclic(ds_usppval)\n",
    "\n",
    "        ds_vsptrend = AddAllCyclic(ds_vsptrend)\n",
    "        ds_vsppval = AddAllCyclic(ds_vsppval)\n",
    "\n",
    "        SaveData(ds_usptrend, graph_type_str, 'U', time_outstr)\n",
    "        SaveData(ds_usppval, graph_type_str+'.pval', 'U', time_outstr)\n",
    "\n",
    "        SaveData(ds_vsptrend, graph_type_str, 'V', time_outstr)\n",
    "        SaveData(ds_vsppval, graph_type_str+'.pval', 'V', time_outstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dda76c-341b-4836-a447-e9ac665cabca",
   "metadata": {},
   "source": [
    "## Spatial plots\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac422a2d-8296-4d90-b149-66a32c8b5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots['map'][0]:\n",
    "    graph_type_str = 'Map'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a9220-a298-4f09-9840-e8d13a8f9d12",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83eed395-be9c-4c37-84f3-0beeb9b67c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['map'][0]:\n",
    "    # Drop CESM2-LENS individual members\n",
    "    if file_bool:\n",
    "        ds_avg = ds_proc.drop_vars(['LENS2 piControl'])\n",
    "    else:\n",
    "        ds_avg = ds_proc\n",
    "    \n",
    "    # Calculating averages\n",
    "    ds_sp = SpatZonAvg(ds_avg)\n",
    "\n",
    "    if var == 'Z3':\n",
    "        ds_spv = SpatZonVar(ds_avg)\n",
    "        ds_usp = SpatZonAvg(ds_u)\n",
    "        ds_vsp = SpatZonAvg(ds_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8a489f1-522b-4183-ae35-cfe011f883ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['map'][0]:\n",
    "    if var == 'aice':\n",
    "        ds_sp = Regrid(ds_sp, regridderSIC, 'sic')\n",
    "        \n",
    "    # Add cyclic data \n",
    "    ds_sp = AddAllCyclic(ds_sp)\n",
    "    SaveData(ds_sp, graph_type_str, var, time_outstr)\n",
    "    \n",
    "    if var == 'Z3':\n",
    "        ds_spv = AddAllCyclic(ds_spv)\n",
    "        ds_usp = AddAllCyclic(ds_usp)\n",
    "        ds_vsp = AddAllCyclic(ds_vsp)\n",
    "\n",
    "        SaveData(ds_spv, graph_type_str+'.var', var, time_outstr)\n",
    "        SaveData(ds_usp, graph_type_str, 'U', time_outstr)\n",
    "        SaveData(ds_vsp, graph_type_str, 'V', time_outstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f3a55",
   "metadata": {},
   "source": [
    "## Zonal trend plots\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1da5002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zonal plots\n",
    "if plot_types['zonal'][0]:\n",
    "    # Monthly\n",
    "    if time_avg == 0:\n",
    "        period = 'time'\n",
    "        date_str = mon_str\n",
    "\n",
    "    # Seasonally\n",
    "    elif time_avg == 2:\n",
    "        period = 'time'\n",
    "        date_str = seas_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c320f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots['ztrd'][0]:\n",
    "    graph_type_str = 'Zonal.Trend'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9d523",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fbae0f2-a933-4b76-a156-969ae41e7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['ztrd'][0]:\n",
    "    # Calculate statistics for LENS2 piControl\n",
    "    ds_lens_stats = LENS2TrendsEnsemble(graph_type_str, ds_proc['LENS2 piControl'], period, var, False)\n",
    "    ds_ulens_stats = LENS2TrendsEnsemble(graph_type_str, ds_u['LENS2 piControl'], period, 'U', False)\n",
    "    ds_vlens_stats = LENS2TrendsEnsemble(graph_type_str, ds_v['LENS2 piControl'], period, 'V', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a20f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['ztrd'][0]:\n",
    "    print('Calculating zonal trends in '+var)\n",
    "    ds_ztrend, ds_zpval = SpatZonTrends(ds_proc, ds_lens_stats, period, var)\n",
    "\n",
    "    if var == 'Z3':\n",
    "        print('Calculating zonal trends in U')\n",
    "        ds_uztrend, ds_uzpval = SpatZonTrends(ds_u, ds_ulens_stats, period, 'U')\n",
    "        print('Calculating zonal trends in V')\n",
    "        ds_vztrend, ds_vzpval = SpatZonTrends(ds_v, ds_vlens_stats, period, 'V')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20775555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['ztrd'][0]:\n",
    "    SaveData(ds_ztrend, graph_type_str, var, time_outstr)\n",
    "    SaveData(ds_zpval, graph_type_str+'.pval', var, time_outstr)\n",
    "\n",
    "    if var == 'Z3':\n",
    "        SaveData(ds_uztrend, graph_type_str, 'U', time_outstr)\n",
    "        SaveData(ds_uzpval, graph_type_str+'.pval', 'U', time_outstr)\n",
    "        \n",
    "        SaveData(ds_vztrend, graph_type_str, 'V', time_outstr)\n",
    "        SaveData(ds_vzpval, graph_type_str+'.pval', 'V', time_outstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115fe6a0",
   "metadata": {},
   "source": [
    "## Zonal Plots\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3135241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots['zon'][0]:\n",
    "    graph_type_str = 'Zonal'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221bce3",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "345e8f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 µs, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['zon'][0]:\n",
    "    # Time averaging\n",
    "    ds_zon = SpatZonAvg(ds_proc)\n",
    "    \n",
    "    if var == 'Z3':\n",
    "        # Calculate for U & V\n",
    "        ds_uzon = SpatZonAvg(ds_u)\n",
    "        ds_vzon = SpatZonAvg(ds_v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0175d735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if plots['zon'][0]:\n",
    "    SaveData(ds_zon, graph_type_str, var, time_outstr)\n",
    "\n",
    "    if var == 'Z3':\n",
    "        SaveData(ds_uzon, graph_type_str, 'U', time_outstr)\n",
    "        SaveData(ds_vzon, graph_type_str, 'V', time_outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9532f3b1-2a3c-4eda-925b-5a2b4e9900aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be6af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cenv]",
   "language": "python",
   "name": "conda-env-cenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
